{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phishing URL Detection - Complete Training Pipeline\n",
        "\n",
        "## \ud83c\udfaf Overview\n",
        "This notebook trains a Random Forest classifier to detect phishing URLs using 19 URL-based features.\n",
        "\n",
        "## \u26a1 Key Features:\n",
        "- **No webpage fetching** - works with just the URL\n",
        "- **19 engineered features** - domain, TLD, path, protocol analysis\n",
        "- **Random Forest model** - balanced, high accuracy\n",
        "- **Fixed TrustedBrandOnHTTP logic** - no false positives on legitimate sites\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    roc_auc_score, \n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "from pathlib import Path\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from urllib.parse import urlparse\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import tldextract\n",
        "try:\n",
        "    import tldextract\n",
        "    HAS_TLDEXTRACT = True\n",
        "    print(\"\u2705 tldextract library found\")\n",
        "except ImportError:\n",
        "    HAS_TLDEXTRACT = False\n",
        "    print(\"\u26a0\ufe0f  tldextract not found - using fallback TLD extraction\")\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"\u2705 All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Feature Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature extraction constants\n",
        "COMMON_LEGIT_TLDS = {\n",
        "    \"com\": 0.9, \"org\": 0.85, \"net\": 0.8, \"edu\": 0.95, \"gov\": 0.97,\n",
        "    \"co\": 0.7, \"uk\": 0.8, \"de\": 0.75, \"fr\": 0.75, \"ca\": 0.8\n",
        "}\n",
        "\n",
        "SUSPICIOUS_TLDS = {\"xyz\", \"tk\", \"ml\", \"ga\", \"cf\", \"ru\", \"cn\", \"top\", \"gq\", \"pw\"}\n",
        "\n",
        "BRANDS = [\n",
        "    \"paypal\", \"google\", \"facebook\", \"amazon\", \"instagram\", \"bank\", \n",
        "    \"sbi\", \"hdfc\", \"icici\", \"apple\", \"microsoft\", \"netflix\", \"ebay\",\n",
        "    \"myntra\", \"flipkart\", \"wikipedia\", \"github\", \"linkedin\", \"twitter\"\n",
        "]\n",
        "\n",
        "PHISHING_KEYWORDS = [\"login\", \"secure\", \"account\", \"verify\", \"bank\", \"update\", \"confirm\"]\n",
        "\n",
        "\n",
        "def simple_tld_extract(url):\n",
        "    \"\"\"Fallback TLD extraction if tldextract is not available\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc or parsed.path.split('/')[0]\n",
        "    domain = domain.split(':')[0]\n",
        "    parts = domain.split('.')\n",
        "    \n",
        "    if len(parts) >= 2:\n",
        "        return {\n",
        "            'domain': parts[-2],\n",
        "            'suffix': parts[-1],\n",
        "            'subdomain': '.'.join(parts[:-2]) if len(parts) > 2 else ''\n",
        "        }\n",
        "    return {'domain': domain, 'suffix': '', 'subdomain': ''}\n",
        "\n",
        "\n",
        "def _normalize_url(url: str) -> str:\n",
        "    \"\"\"Normalize URL so trailing slash doesn't change features.\"\"\"\n",
        "    url = url.strip()\n",
        "    parsed = urlparse(url)\n",
        "    if parsed.path == \"/\" and not parsed.query and not parsed.fragment:\n",
        "        url = url.rstrip(\"/\")\n",
        "    return url\n",
        "\n",
        "\n",
        "def extract_features(url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract URL-based features for phishing detection.\n",
        "    Returns 19 features that can be extracted without fetching the webpage.\n",
        "    \"\"\"\n",
        "    if not url or not isinstance(url, str):\n",
        "        url = str(url) if url else \"\"\n",
        "    \n",
        "    url = _normalize_url(url)\n",
        "    \n",
        "    # Parse URL\n",
        "    if HAS_TLDEXTRACT:\n",
        "        ext = tldextract.extract(url)\n",
        "        domain = ext.domain\n",
        "        tld = ext.suffix.lower()\n",
        "        subdomain = ext.subdomain\n",
        "    else:\n",
        "        ext_dict = simple_tld_extract(url)\n",
        "        domain = ext_dict['domain']\n",
        "        tld = ext_dict['suffix'].lower()\n",
        "        subdomain = ext_dict['subdomain']\n",
        "    \n",
        "    parsed = urlparse(url)\n",
        "    \n",
        "    # Character repetition\n",
        "    repeated_chars = sum(1 for i in range(1, len(url)) if url[i] == url[i - 1])\n",
        "    \n",
        "    # Similarity to phishing keywords\n",
        "    max_similarity = max(\n",
        "        SequenceMatcher(None, url.lower(), kw).ratio()\n",
        "        for kw in PHISHING_KEYWORDS\n",
        "    )\n",
        "    url_similarity_index = max_similarity * 100\n",
        "    \n",
        "    # Protocol check\n",
        "    has_https = 1 if parsed.scheme == \"https\" else 0\n",
        "    \n",
        "    # Subdomain analysis\n",
        "    subdomain_count = len(subdomain.split('.')) if subdomain else 0\n",
        "    \n",
        "    # Character analysis\n",
        "    digit_count = sum(c.isdigit() for c in url)\n",
        "    special_char_count = sum(c in \"@?=-_&\" for c in url)\n",
        "    \n",
        "    # IP address check\n",
        "    is_ip = bool(re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain))\n",
        "    \n",
        "    # Path analysis\n",
        "    path_length = len(parsed.path) if parsed.path else 0\n",
        "    path_depth = parsed.path.count('/') if parsed.path else 0\n",
        "    \n",
        "    # Suspicious patterns\n",
        "    has_at_symbol = 1 if '@' in url else 0\n",
        "    double_slash_redirecting = 1 if url.count('//') > 1 else 0\n",
        "\n",
        "    # Known brand check\n",
        "    has_brand = int(any(b in domain.lower() for b in BRANDS))\n",
        "    \n",
        "    # FIXED: TrustedBrandOnHTTP - only flag suspicious subdomain usage\n",
        "    has_suspicious_subdomain = subdomain and any(b in subdomain.lower() for b in BRANDS)\n",
        "    trusted_brand_on_http = 1 if (has_suspicious_subdomain and has_https == 0) else 0\n",
        "\n",
        "    return {\n",
        "        \"URLLength\": len(url),\n",
        "        \"DomainLength\": len(domain),\n",
        "        \"IsDomainIP\": int(is_ip),\n",
        "        \"URLSimilarityIndex\": url_similarity_index,\n",
        "        \"CharContinuationRate\": repeated_chars / max(len(url), 1),\n",
        "        \"TLDLegitimateProb\": COMMON_LEGIT_TLDS.get(tld, 0.05),\n",
        "        \"HasBrandName\": has_brand,\n",
        "        \"HyphenCount\": domain.count(\"-\"),\n",
        "        \"SuspiciousTLD\": int(tld in SUSPICIOUS_TLDS),\n",
        "        \"HasHTTPS\": has_https,\n",
        "        \"TrustedBrandOnHTTP\": trusted_brand_on_http,\n",
        "        \"SubdomainLevel\": subdomain_count,\n",
        "        \"PathLength\": path_length,\n",
        "        \"PathDepth\": path_depth,\n",
        "        \"DigitCount\": digit_count,\n",
        "        \"SpecialCharCount\": special_char_count,\n",
        "        \"HasAtSymbol\": has_at_symbol,\n",
        "        \"DoubleSlashRedirecting\": double_slash_redirecting,\n",
        "        \"PrefixSuffix\": 1 if '-' in domain else 0,\n",
        "    }\n",
        "\n",
        "print(\"\u2705 Feature extraction functions defined\")\n",
        "print(f\"   Total features: 19\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset\n",
        "\n",
        "Make sure you have the dataset at: `../data/raw/PhiUSIIL_Phishing_URL_Dataset.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "DATA_PATH = Path(\"../data/raw/PhiUSIIL_Phishing_URL_Dataset.csv\")\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    print(f\"\u274c Dataset not found at: {DATA_PATH.absolute()}\")\n",
        "    print(\"   Please place the dataset file there and try again.\")\n",
        "else:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"\u2705 Dataset loaded from: {DATA_PATH}\")\n",
        "    print(f\"\\nDataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns[:10])}...\")\n",
        "    print(f\"\\nClass distribution:\")\n",
        "    print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Features from URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features from all URLs\n",
        "print(\"Extracting features from URLs...\")\n",
        "feature_list = []\n",
        "for url in df['URL']:\n",
        "    features = extract_features(url)\n",
        "    feature_list.append(features)\n",
        "\n",
        "# Create feature DataFrame\n",
        "X = pd.DataFrame(feature_list)\n",
        "y = df['label']\n",
        "\n",
        "print(f\"\\n\u2705 Features extracted: {X.shape}\")\n",
        "print(f\"   Features: {list(X.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest\n",
        "print(\"Training Random Forest model...\")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\u2705 Model trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Phishing']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 10 Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory in project root\n",
        "MODEL_PATH = Path(\"../models/url_rf_model.pkl\")\n",
        "MODEL_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Create model bundle with metadata\n",
        "model_bundle = {\n",
        "    \"model\": rf_model,\n",
        "    \"features\": list(X.columns),\n",
        "    \"accuracy\": accuracy,\n",
        "    \"f1_score\": f1,\n",
        "    \"roc_auc\": roc_auc,\n",
        "}\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model_bundle, MODEL_PATH)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL SAVED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Location: {MODEL_PATH.absolute()}\")\n",
        "print(f\"File size: {MODEL_PATH.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test on Sample URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test URLs\n",
        "test_urls = [\n",
        "    (\"https://www.google.com\", \"LEGITIMATE\"),\n",
        "    (\"https://www.github.com\", \"LEGITIMATE\"),\n",
        "    (\"http://www.amazon.com\", \"LEGITIMATE\"),\n",
        "    (\"http://secure-paypal-login.xyz\", \"PHISHING\"),\n",
        "    (\"https://192.168.1.1/login\", \"PHISHING\"),\n",
        "    (\"http://paypal-secure.verify-account.ml\", \"PHISHING\"),\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING ON SAMPLE URLs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "results = []\n",
        "for url, expected in test_urls:\n",
        "    features = extract_features(url)\n",
        "    X_url = pd.DataFrame([features])[list(X.columns)]\n",
        "    \n",
        "    pred_proba = rf_model.predict_proba(X_url)[0][1]\n",
        "    pred = \"PHISHING\" if pred_proba >= 0.5 else \"LEGITIMATE\"\n",
        "    \n",
        "    status = \"\u2705\" if pred == expected else \"\u274c\"\n",
        "    results.append((status, url, expected, pred, pred_proba))\n",
        "    \n",
        "    print(f\"\\n{status} URL: {url}\")\n",
        "    print(f\"   Expected: {expected}, Predicted: {pred} (Risk: {pred_proba:.4f})\")\n",
        "\n",
        "correct = sum(1 for r in results if r[0] == \"\u2705\")\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(f\"Test Results: {correct}/{len(results)} correct ({correct/len(results)*100:.1f}%)\")\n",
        "print(f\"{'=' * 70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Use the Model for Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and predict\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from features import extract_features\n",
        "\n",
        "# Load the model\n",
        "MODEL_PATH = Path(\"../models/url_rf_model.pkl\")\n",
        "model_bundle = joblib.load(MODEL_PATH)\n",
        "\n",
        "# Test it\n",
        "url = \"https://www.google.com\"\n",
        "features = extract_features(url)\n",
        "X = pd.DataFrame([features])[model_bundle[\"features\"]]\n",
        "risk_score = model_bundle[\"model\"].predict_proba(X)[0][1]\n",
        "prediction = \"PHISHING\" if risk_score >= 0.5 else \"LEGITIMATE\"\n",
        "\n",
        "print(f\"URL: {url}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"Risk Score: {risk_score:.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"For interactive prediction, run from terminal:\")\n",
        "print(\"  cd src && python predict_url.py -i\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}